{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22d18e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashton/miniconda3/envs/SWM/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "from datasets import load_from_disk, Dataset\n",
    "\n",
    "import stable_worldmodel as swm\n",
    "from stable_worldmodel.data import StepsDataset, dataset_info\n",
    "from stable_worldmodel.policy import AutoCostModel\n",
    "from stable_worldmodel.wm.dinowm import DINOWM\n",
    "\n",
    "from PIL import Image\n",
    "import stable_pretraining as spt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import math, time, contextlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9a2f7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "NUM_WORKERS = 6\n",
    "NUM_STEPS = 2 # T\n",
    "BATCH_SIZE = 256 # B\n",
    "FRAMESKIP = 5 # S\n",
    "EPOCHS = 25\n",
    "LEARNING_RATE = 3e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "USE_ACTIONS = True\n",
    "\n",
    "# file paths\n",
    "TRAIN_DIR = \"pusht_expert_dataset_train\"\n",
    "VAL_DIR = \"pusht_expert_dataset_val\"\n",
    "CHECKPOINT_NAME = \"dinowm_pusht_object.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80b6cff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple MLP\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, out_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4d4dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian MLP - outputs distribution over actions\n",
    "class GaussianMLP(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, dropout_prob=0.1, hidden_dim=512, \n",
    "                 feature_dim=256, head_hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.out_dim = out_dim\n",
    "        \n",
    "        # Shared feature extractor backbone\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Linear(hidden_dim, feature_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "        )\n",
    "        \n",
    "        # Separate MLP head for mean\n",
    "        self.mean_head = nn.Sequential(\n",
    "            nn.Linear(feature_dim, head_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Linear(head_hidden_dim, out_dim)\n",
    "        )\n",
    "        \n",
    "        # Separate MLP head for log_std\n",
    "        self.log_std_head = nn.Sequential(\n",
    "            nn.Linear(feature_dim, head_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Linear(head_hidden_dim, out_dim)\n",
    "        )\n",
    "        \n",
    "        # Initialize log_std head output to reasonable values\n",
    "        nn.init.constant_(self.log_std_head[-1].bias, -0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        mean = self.mean_head(features)\n",
    "        log_std = self.log_std_head(features)\n",
    "        log_std = torch.clamp(log_std, min=-10, max=2)\n",
    "        return mean, log_std\n",
    "    \n",
    "    def log_prob(self, mean, log_std, action):\n",
    "        std = torch.exp(log_std)\n",
    "        var = std ** 2\n",
    "        log_prob = -0.5 * (\n",
    "            ((action - mean) ** 2) / var +\n",
    "            2 * log_std +\n",
    "            math.log(2 * math.pi)\n",
    "        )\n",
    "        return log_prob.sum(dim=-1)\n",
    "    \n",
    "    def sample(self, mean, log_std):\n",
    "        std = torch.exp(log_std)\n",
    "        eps = torch.randn_like(mean)\n",
    "        return mean + eps * std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e1ce5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian MLP - outputs distribution over actions\n",
    "class GaussianMLP(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, dropout_prob=0.1, hidden_dim=512, feature_dim=256, head_hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.out_dim = out_dim\n",
    "        \n",
    "        # Shared feature extractor backbone\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Linear(hidden_dim, feature_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "        )\n",
    "        \n",
    "        # Separate MLP head for mean\n",
    "        self.mean_head = nn.Sequential(\n",
    "            nn.Linear(feature_dim, head_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Linear(head_hidden_dim, out_dim)\n",
    "        )\n",
    "        \n",
    "        # Separate MLP head for log_std\n",
    "        self.log_std_head = nn.Sequential(\n",
    "            nn.Linear(feature_dim, head_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Linear(head_hidden_dim, out_dim)\n",
    "        )\n",
    "        \n",
    "        # Initialize log_std head output to reasonable values (e.g., log(0.5) ≈ -0.69)\n",
    "        nn.init.constant_(self.log_std_head[-1].bias, -0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            mean: predicted mean of action distribution [B, out_dim]\n",
    "            log_std: predicted log standard deviation [B, out_dim]\n",
    "        \"\"\"\n",
    "        # Extract shared features\n",
    "        features = self.feature_extractor(x)\n",
    "        \n",
    "        # Pass through separate MLP heads\n",
    "        mean = self.mean_head(features)\n",
    "        log_std = self.log_std_head(features)\n",
    "        \n",
    "        # Clamp log_std for numerical stability\n",
    "        log_std = torch.clamp(log_std, min=-10, max=2)\n",
    "        \n",
    "        return mean, log_std\n",
    "    \n",
    "    def log_prob(self, mean, log_std, action):\n",
    "        \"\"\"\n",
    "        Compute log probability of action under Gaussian distribution.\n",
    "        \n",
    "        Args:\n",
    "            mean: predicted mean [B, out_dim]\n",
    "            log_std: predicted log std [B, out_dim]\n",
    "            action: ground truth action [B, out_dim]\n",
    "            \n",
    "        Returns:\n",
    "            log_prob: log probability [B]\n",
    "        \"\"\"\n",
    "        std = torch.exp(log_std)\n",
    "        var = std ** 2\n",
    "        \n",
    "        # Gaussian log probability: -0.5 * [(x-μ)²/σ² + log(2πσ²)]\n",
    "        log_prob = -0.5 * (\n",
    "            ((action - mean) ** 2) / var +\n",
    "            2 * log_std +\n",
    "            math.log(2 * math.pi)\n",
    "        )\n",
    "        \n",
    "        # Sum over action dimensions\n",
    "        return log_prob.sum(dim=-1)\n",
    "    \n",
    "    def sample(self, mean, log_std):\n",
    "        \"\"\"\n",
    "        Sample action from the predicted distribution.\n",
    "        \n",
    "        Args:\n",
    "            mean: predicted mean [B, out_dim]\n",
    "            log_std: predicted log std [B, out_dim]\n",
    "            \n",
    "        Returns:\n",
    "            action: sampled action [B, out_dim]\n",
    "        \"\"\"\n",
    "        std = torch.exp(log_std)\n",
    "        eps = torch.randn_like(mean)\n",
    "        return mean + eps * std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff54e519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform function: normalize + reshape\n",
    "def make_transform(keys):\n",
    "    transforms = []\n",
    "    for key in keys:\n",
    "        transforms.append(\n",
    "            spt.data.transforms.Compose(\n",
    "                spt.data.transforms.ToImage(\n",
    "                    mean=[0.5, 0.5, 0.5],\n",
    "                    std=[0.5, 0.5, 0.5],\n",
    "                    source=key,\n",
    "                    target=key,\n",
    "                ),\n",
    "                spt.data.transforms.Resize(224, source=key, target=key),\n",
    "                spt.data.transforms.CenterCrop(224, source=key, target=key),\n",
    "            )\n",
    "        )\n",
    "    return spt.data.transforms.Compose(*transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eac2c2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def load_dataset(dir, num_steps=1, frameskip=FRAMESKIP):\n",
    "    transform = make_transform([f'pixels.{i}' for i in range(num_steps)])\n",
    "\n",
    "    dataset = StepsDataset(dir, num_steps=num_steps, transform=transform, frameskip=frameskip) # frameskip?\n",
    "\n",
    "    # hacky\n",
    "    dataset.data_dir = dataset.data_dir.parent\n",
    "\n",
    "    # add goal column if not there\n",
    "    goals = cache_goals(dataset)\n",
    "\n",
    "    return dataset, goals\n",
    "\n",
    "def cache_goals(steps: StepsDataset):\n",
    "    data = steps.dataset.with_format(\"python\")\n",
    "    goals = {} # {episode -> {goal pixel, goal_proprio}}\n",
    "    transform = make_transform(['goal_pixels'])\n",
    "\n",
    "    for ep, indices in steps.episode_slices.items():\n",
    "        goal_idx = indices[-1]\n",
    "        goal_px_path = steps.data_dir / data[\"pixels\"][goal_idx]\n",
    "        # transform the goal here\n",
    "        with Image.open(goal_px_path) as img:\n",
    "            pixels = {'goal_pixels': img.convert('RGB')}\n",
    "            transform(pixels)\n",
    "            goal_pixels = pixels['goal_pixels']\n",
    "        goals[ep] = {\"goal_pixels\": goal_pixels,\n",
    "                     \"goal_proprio\": torch.as_tensor(data[\"proprio\"][goal_idx]),\n",
    "                     }\n",
    "\n",
    "    steps.dataset = data.with_format(\"torch\")\n",
    "    return goals\n",
    "\n",
    "def attach_goals(batch, goals):\n",
    "    goal_pixels = []\n",
    "    goal_proprios = []\n",
    "\n",
    "    for ep in batch[\"episode_idx\"].tolist():\n",
    "        goal = goals[ep[0]]\n",
    "        goal_pixels.append(goal[\"goal_pixels\"])\n",
    "        goal_proprios.append(goal[\"goal_proprio\"])\n",
    "\n",
    "    batch[\"goal_pixels\"] = torch.stack(goal_pixels).unsqueeze(1)\n",
    "    batch[\"goal_proprio\"] = torch.stack(goal_proprios).unsqueeze(1)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd71de68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(train_data, val_data, batch_size, device, num_workers):\n",
    "    # optionally pin_memory on CUDA, not Mac\n",
    "    train_loader = DataLoader(\n",
    "        train_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        persistent_workers=True,\n",
    "        pin_memory=(device.type=='cuda')\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        persistent_workers=True,\n",
    "        pin_memory=(device.type=='cuda')\n",
    "    )\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb078b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model checkpoint from cache_dir in inference mode\n",
    "def load_checkpoint(checkpoint_name, device):\n",
    "    cache_dir = swm.data.get_cache_dir()\n",
    "    checkpoint_path = cache_dir / checkpoint_name\n",
    "    model = torch.load(checkpoint_path, map_location=device, weights_only=False) # weights_only false\n",
    "    model = model.to(device).eval()\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad_(False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6cd71f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder func: call dinowm encoders\n",
    "@torch.inference_mode()\n",
    "def encode(batch, dinowm, device, use_actions=False):\n",
    "    # this is hacky\n",
    "    pixels = torch.cat((batch[\"pixels\"],\n",
    "                        batch['goal_pixels']), dim=1).to(device, non_blocking=True)\n",
    "    proprio = torch.cat((batch[\"proprio\"],\n",
    "                         batch[\"goal_proprio\"]), dim=1).to(device, non_blocking=True)\n",
    "    actions = (\n",
    "        torch.cat(\n",
    "            (batch[\"action\"], torch.zeros_like(batch['action'][:, :1])),\n",
    "            dim=1,\n",
    "        ).to(device, non_blocking=True) if use_actions else None\n",
    "    ) # pad with a single step of 0s\n",
    "\n",
    "    data = {\n",
    "        \"pixels\": pixels,\n",
    "        \"proprio\": proprio,\n",
    "        \"action\": actions,\n",
    "    }\n",
    "\n",
    "    context = torch.autocast(device_type=device.type, dtype=torch.float16) if device.type in (\"cuda\", \"mps\") else contextlib.nullcontext()\n",
    "    with context:\n",
    "        out = dinowm.encode(\n",
    "            data,\n",
    "            target=\"embed\",\n",
    "            pixels_key=\"pixels\",\n",
    "            proprio_key=\"proprio\",\n",
    "            action_key=(\"action\" if use_actions else None),\n",
    "        )\n",
    "\n",
    "    # attach attention pooler here\n",
    "    pix_out = out[\"pixels_embed\"].mean(dim=2).float()\n",
    "    prp_out = out[\"proprio_embed\"].float()\n",
    "\n",
    "    # detach goal pixels + proprio\n",
    "    z_pix, z_gpix = pix_out[:,:-1], pix_out[:,-1] # B x T x d_pixels (pooled by patch), B x d_pixels\n",
    "    z_prp, z_gprp = prp_out[:,:-1], prp_out[:,-1] # B x T x d_proprio, B x d_proprio\n",
    "    \n",
    "    z_act = None\n",
    "    if use_actions:\n",
    "        z_act = out[\"action_embed\"][:,:-2].float() # B x (T - 1) * d_actions_effective := (d_actions * frame_skip)\n",
    "    \n",
    "    return z_pix, z_prp, z_act, z_gpix, z_gprp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e46e05b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def to_feature(z_pix, z_prp, z_act, z_gpix, z_gprp):\n",
    "    parts = [z_pix[:,:-1], z_prp[:,:-1]]\n",
    "    if z_act is not None:\n",
    "        parts.append(z_act)\n",
    "\n",
    "    # history latents (block of S actions + state := (pixel + proprio embeddings))\n",
    "    z_hist = torch.cat(parts, dim=2) # B x (T-1) x d_embed := [d_pixels + d_proprio (+ d_actions_effective)]\n",
    "    z_hist = torch.flatten(z_hist, start_dim=1, end_dim=2) # B x ((T-1) * d_embed)\n",
    "    \n",
    "    # current + goal latents (just pixel + proprio embeedings)\n",
    "    z_cur = torch.cat((z_pix[:,-1], z_prp[:,-1], z_gpix, z_gprp), dim=1) # B x 2 * (d_pixels + d_proprio)\n",
    "\n",
    "    # concat\n",
    "    z = torch.cat((z_hist, z_cur), dim=1)\n",
    "    return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e628f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: mps\n",
      "\u001b[32m17:56:22.114\u001b[0m | \u001b[33m\u001b[1mWARNING\u001b[0m (\u001b[36m74564, stable_pretraining.data.datasets\u001b[0m) | \u001b[33m\u001b[1mYou didn't pass a storage optionwe are adding one to avoid timeout\u001b[0m\n",
      "\u001b[32m17:56:22.114\u001b[0m | \u001b[1mINFO   \u001b[0m (\u001b[36m74564, stable_pretraining.data.datasets\u001b[0m) | \u001b[1mLoading dataset with load_from_disk /Users/ashton/.stable_worldmodel/pusht_expert_dataset_train\u001b[0m\n",
      "\u001b[32m17:58:05.019\u001b[0m | \u001b[33m\u001b[1mWARNING\u001b[0m (\u001b[36m74564, stable_pretraining.data.datasets\u001b[0m) | \u001b[33m\u001b[1mYou didn't pass a storage optionwe are adding one to avoid timeout\u001b[0m\n",
      "\u001b[32m17:58:05.021\u001b[0m | \u001b[1mINFO   \u001b[0m (\u001b[36m74564, stable_pretraining.data.datasets\u001b[0m) | \u001b[1mLoading dataset with load_from_disk /Users/ashton/.stable_worldmodel/pusht_expert_dataset_val\u001b[0m\n",
      "Loaded datasets: train size=2168571, val size=2325\n",
      "Loaded DINO-WM from checkpoint: 'dinowm_pusht_object.ckpt'\n",
      "latent_dim=1192, action_dim=2\n"
     ]
    }
   ],
   "source": [
    "# find device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "def sync():\n",
    "    if device.type=='cuda':\n",
    "        torch.cuda.synchronize()\n",
    "    if device.type=='mps':\n",
    "        torch.mps.synchronize()\n",
    "\n",
    "print(\"device:\", device)\n",
    "\n",
    "# load datasets\n",
    "train_data, train_goals  = load_dataset(TRAIN_DIR, NUM_STEPS)\n",
    "val_data, val_goals = load_dataset(VAL_DIR, NUM_STEPS)\n",
    "print(f\"Loaded datasets: train size={len(train_data)}, val size={len(val_data)}\")\n",
    "\n",
    "# build loaders\n",
    "train_loader, val_loader = get_loaders(train_data, val_data, BATCH_SIZE, device, NUM_WORKERS)\n",
    "\n",
    "# load DINO-WM\n",
    "dinowm = load_checkpoint(CHECKPOINT_NAME, device)\n",
    "assert isinstance(dinowm, DINOWM)\n",
    "print(f\"Loaded DINO-WM from checkpoint: '{CHECKPOINT_NAME}'\")\n",
    "    \n",
    "# calculate dims\n",
    "d_pixel = dinowm.backbone.config.hidden_size\n",
    "d_proprio = dinowm.proprio_encoder.emb_dim\n",
    "d_action = dinowm.action_encoder.emb_dim\n",
    "\n",
    "LATENT_DIM = (d_pixel + d_proprio) * (NUM_STEPS + 1) + (d_action if USE_ACTIONS else 0) * (NUM_STEPS - 1)\n",
    "ACTION_DIM = 2 # predict actual action, not latent\n",
    "print(f'latent_dim={LATENT_DIM}, action_dim={ACTION_DIM}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "922f669e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['episode_idx', 'step_idx', 'action', 'state', 'proprio', 'pixels', 'sample_idx']\n"
     ]
    }
   ],
   "source": [
    "print(train_data.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa2ea63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Gaussian MLP with negative log likelihood loss\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Training Gaussian MLP\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "gaussian_action_head = GaussianMLP(LATENT_DIM, ACTION_DIM, dropout_prob=0.15).to(device)\n",
    "optimizer_gaussian = torch.optim.AdamW(\n",
    "    gaussian_action_head.parameters(), \n",
    "    lr=LEARNING_RATE, \n",
    "    weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    gaussian_action_head.train()\n",
    "    epoch_nll = 0.0\n",
    "    num_samples = 0\n",
    "    \n",
    "    t0 = time.perf_counter()\n",
    "    n = 0\n",
    "    num_batches = len(train_loader)\n",
    "    \n",
    "    for i, batch in enumerate(train_loader):\n",
    "        attach_goals(batch, train_goals)\n",
    "        z_pix, z_prp, z_act, z_gpix, z_gprp = encode(batch, dinowm, device, USE_ACTIONS)\n",
    "        z = to_feature(z_pix, z_prp, z_act, z_gpix, z_gprp)\n",
    "        action = batch['action'][:,-1,:2].to(device)\n",
    "        \n",
    "        # Forward pass - get mean and log_std\n",
    "        mean, log_std = gaussian_action_head(z)\n",
    "        \n",
    "        # Compute negative log likelihood loss\n",
    "        log_prob = gaussian_action_head.log_prob(mean, log_std, action)\n",
    "        nll_loss = -log_prob.mean()\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer_gaussian.zero_grad()\n",
    "        nll_loss.backward()\n",
    "        optimizer_gaussian.step()\n",
    "        \n",
    "        # Track stats\n",
    "        batch_size = action.shape[0]\n",
    "        epoch_nll += nll_loss.item() * batch_size\n",
    "        num_samples += batch_size\n",
    "        n += batch_size\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            sync()\n",
    "            elapsed = time.perf_counter() - t0\n",
    "            sps = i / max(1e-9, elapsed)\n",
    "            eta = (num_batches - i) / max(1e-9, sps)\n",
    "            print(f\"Epoch {epoch}: step {i}/{num_batches}, NLL={nll_loss.item():.4f}, ETA={eta/60:.1f}min\")\n",
    "    \n",
    "    # Validation\n",
    "    gaussian_action_head.eval()\n",
    "    val_nll = 0.0\n",
    "    val_mse = 0.0\n",
    "    val_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            attach_goals(batch, val_goals)\n",
    "            z_pix, z_prp, z_act, z_gpix, z_gprp = encode(batch, dinowm, device, USE_ACTIONS)\n",
    "            z = to_feature(z_pix, z_prp, z_act, z_gpix, z_gprp)\n",
    "            action = batch['action'][:,-1,:2].to(device)\n",
    "            \n",
    "            mean, log_std = gaussian_action_head(z)\n",
    "            log_prob = gaussian_action_head.log_prob(mean, log_std, action)\n",
    "            nll = -log_prob.mean()\n",
    "            mse = F.mse_loss(mean, action, reduction='sum')\n",
    "            \n",
    "            batch_size = action.shape[0]\n",
    "            val_nll += nll.item() * batch_size\n",
    "            val_mse += mse.item()\n",
    "            val_samples += batch_size\n",
    "    \n",
    "    avg_train_nll = epoch_nll / num_samples\n",
    "    avg_val_nll = val_nll / val_samples\n",
    "    val_rmse = math.sqrt(val_mse / val_samples)\n",
    "    \n",
    "    print(f'\\nEpoch {epoch} Summary:')\n",
    "    print(f'  Train NLL: {avg_train_nll:.6f}')\n",
    "    print(f'  Val NLL:   {avg_val_nll:.6f}')\n",
    "    print(f'  Val RMSE (mean only): {val_rmse:.6f}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f089130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained models\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Saving Models\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "torch.save(action_head.state_dict(), \"mlp_action_head.pt\")\n",
    "print(\"✓ Saved MLP model to: mlp_action_head.pt\")\n",
    "\n",
    "torch.save(gaussian_action_head.state_dict(), \"gaussian_mlp_action_head.pt\")\n",
    "print(\"✓ Saved Gaussian MLP model to: gaussian_mlp_action_head.pt\")\n",
    "\n",
    "print(\"\\nModels saved! Ready to test with test_mlp_policies.py\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4684008c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Using the Gaussian MLP for inference\n",
    "# This shows how to get both deterministic (mean) and stochastic (sampled) actions\n",
    "\n",
    "gaussian_action_head.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Get a single batch for demonstration\n",
    "    batch = next(iter(val_loader))\n",
    "    attach_goals(batch, val_goals)\n",
    "    \n",
    "    # Encode\n",
    "    z_pix, z_prp, z_act, z_gpix, z_gprp = encode(batch, dinowm, device, USE_ACTIONS)\n",
    "    z = to_feature(z_pix, z_prp, z_act, z_gpix, z_gprp)\n",
    "    action_gt = batch['action'][:,-1,:2].to(device)\n",
    "    \n",
    "    # Get distribution parameters\n",
    "    mean, log_std = gaussian_action_head(z)\n",
    "    std = torch.exp(log_std)\n",
    "    \n",
    "    # Sample multiple actions from the distribution\n",
    "    num_samples = 5\n",
    "    sampled_actions = [gaussian_action_head.sample(mean, log_std) for _ in range(num_samples)]\n",
    "    \n",
    "    # Print results for first example in batch\n",
    "    print(\"Example Predictions for First State in Batch:\")\n",
    "    print(f\"Ground Truth Action: {action_gt[0].cpu().numpy()}\")\n",
    "    print(f\"Predicted Mean:      {mean[0].cpu().numpy()}\")\n",
    "    print(f\"Predicted Std:       {std[0].cpu().numpy()}\")\n",
    "    print(f\"\\nSampled Actions:\")\n",
    "    for i, sampled in enumerate(sampled_actions):\n",
    "        print(f\"  Sample {i+1}: {sampled[0].cpu().numpy()}\")\n",
    "    \n",
    "    # Compute log probability of ground truth under learned distribution\n",
    "    log_prob = gaussian_action_head.log_prob(mean, log_std, action_gt)\n",
    "    print(f\"\\nLog Probability of Ground Truth: {log_prob[0].item():.4f}\")\n",
    "    \n",
    "    # Use mean for deterministic prediction (best estimate)\n",
    "    deterministic_action = mean\n",
    "    print(f\"\\nFor deployment, you can use:\")\n",
    "    print(f\"  - Deterministic (mean): {deterministic_action[0].cpu().numpy()}\")\n",
    "    print(f\"  - Stochastic (sample):  {sampled_actions[0][0].cpu().numpy()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63671b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: step 0/8471 Loss = 9.8604 steps / sec = 0.0, batches / sec = 8.5 ETA = 141183333333.3 min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     26\u001b[0m     attach_goals(batch, train_goals)\n\u001b[0;32m---> 27\u001b[0m     z_pix, z_prp, z_act, z_gpix, z_gprp \u001b[38;5;241m=\u001b[39m \u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdinowm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mUSE_ACTIONS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     z \u001b[38;5;241m=\u001b[39m to_feature(z_pix, z_prp, z_act, z_gpix, z_gprp)\n\u001b[1;32m     29\u001b[0m     action \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m'\u001b[39m][:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,:\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m# first action from the last (current) step\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/SWM/lib/python3.10/site-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 24\u001b[0m, in \u001b[0;36mencode\u001b[0;34m(batch, dinowm, device, use_actions)\u001b[0m\n\u001b[1;32m     22\u001b[0m context \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautocast(device_type\u001b[38;5;241m=\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16) \u001b[38;5;28;01mif\u001b[39;00m device\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext()\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context:\n\u001b[0;32m---> 24\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mdinowm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43membed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpixels_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpixels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproprio_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mproprio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43maction_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muse_actions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# attach attention pooler here\u001b[39;00m\n\u001b[1;32m     33\u001b[0m pix_out \u001b[38;5;241m=\u001b[39m out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixels_embed\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n",
      "File \u001b[0;32m~/Desktop/Desktop - Ashton’s MacBook Air/code/stable-worldmodel/stable_worldmodel/wm/dinowm.py:56\u001b[0m, in \u001b[0;36mDINOWM.encode\u001b[0;34m(self, info, pixels_key, target, proprio_key, action_key)\u001b[0m\n\u001b[1;32m     54\u001b[0m pixels \u001b[38;5;241m=\u001b[39m rearrange(pixels, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb t ... -> (b t) ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     55\u001b[0m pixels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_transform(pixels)\n\u001b[0;32m---> 56\u001b[0m pixels_embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixels\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlast_hidden_state\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m     57\u001b[0m pixels_embed \u001b[38;5;241m=\u001b[39m pixels_embed[:, \u001b[38;5;241m1\u001b[39m:, :]  \u001b[38;5;66;03m# drop cls token\u001b[39;00m\n\u001b[1;32m     58\u001b[0m pixels_embed \u001b[38;5;241m=\u001b[39m rearrange(pixels_embed, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(b t) p d -> b t p d\u001b[39m\u001b[38;5;124m\"\u001b[39m, b\u001b[38;5;241m=\u001b[39mB)\n",
      "File \u001b[0;32m~/miniconda3/envs/SWM/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/SWM/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/SWM/lib/python3.10/site-packages/transformers/utils/generic.py:1064\u001b[0m, in \u001b[0;36mcheck_model_inputs.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1061\u001b[0m                 monkey_patched_layers\u001b[38;5;241m.\u001b[39mappend((module, original_forward))\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1064\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[1;32m   1066\u001b[0m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[1;32m   1067\u001b[0m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m     kwargs_without_recordable \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "File \u001b[0;32m~/miniconda3/envs/SWM/lib/python3.10/site-packages/transformers/models/dinov2/modeling_dinov2.py:534\u001b[0m, in \u001b[0;36mDinov2Model.forward\u001b[0;34m(self, pixel_values, bool_masked_pos, head_mask, output_hidden_states, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    532\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(pixel_values, bool_masked_pos\u001b[38;5;241m=\u001b[39mbool_masked_pos)\n\u001b[0;32m--> 534\u001b[0m encoder_outputs: BaseModelOutput \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    538\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm(sequence_output)\n",
      "File \u001b[0;32m~/miniconda3/envs/SWM/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/SWM/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/SWM/lib/python3.10/site-packages/transformers/models/dinov2/modeling_dinov2.py:422\u001b[0m, in \u001b[0;36mDinov2Encoder.forward\u001b[0;34m(self, hidden_states, head_mask, output_hidden_states)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, layer_module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer):\n\u001b[1;32m    421\u001b[0m     layer_head_mask \u001b[38;5;241m=\u001b[39m head_mask[i] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 422\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m all_hidden_states:\n\u001b[1;32m    424\u001b[0m         all_hidden_states\u001b[38;5;241m.\u001b[39mappend(hidden_states)\n",
      "File \u001b[0;32m~/miniconda3/envs/SWM/lib/python3.10/site-packages/transformers/modeling_layers.py:94\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning_once(message)\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/SWM/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/SWM/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/SWM/lib/python3.10/site-packages/transformers/models/dinov2/modeling_dinov2.py:392\u001b[0m, in \u001b[0;36mDinov2Layer.forward\u001b[0;34m(self, hidden_states, head_mask)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    388\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    389\u001b[0m     head_mask: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    390\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    391\u001b[0m     hidden_states_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(hidden_states)\n\u001b[0;32m--> 392\u001b[0m     self_attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    393\u001b[0m     self_attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_scale1(self_attention_output)\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;66;03m# first residual connection\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/SWM/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/SWM/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/SWM/lib/python3.10/site-packages/transformers/models/dinov2/modeling_dinov2.py:281\u001b[0m, in \u001b[0;36mDinov2Attention.forward\u001b[0;34m(self, hidden_states, head_mask)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, head_mask: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 281\u001b[0m     self_attn_output, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_attn_output, hidden_states)\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/miniconda3/envs/SWM/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/SWM/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/SWM/lib/python3.10/site-packages/transformers/models/dinov2/modeling_dinov2.py:219\u001b[0m, in \u001b[0;36mDinov2SelfAttention.forward\u001b[0;34m(self, hidden_states, head_mask)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_attn_implementation \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meager\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    217\u001b[0m     attention_interface \u001b[38;5;241m=\u001b[39m ALL_ATTENTION_FUNCTIONS[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_attn_implementation]\n\u001b[0;32m--> 219\u001b[0m context_layer, attention_probs \u001b[38;5;241m=\u001b[39m \u001b[43mattention_interface\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscaling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout_prob\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m new_context_layer_shape \u001b[38;5;241m=\u001b[39m context_layer\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_head_size,)\n\u001b[1;32m    231\u001b[0m context_layer \u001b[38;5;241m=\u001b[39m context_layer\u001b[38;5;241m.\u001b[39mreshape(new_context_layer_shape)\n",
      "File \u001b[0;32m~/miniconda3/envs/SWM/lib/python3.10/site-packages/transformers/integrations/sdpa_attention.py:96\u001b[0m, in \u001b[0;36msdpa_attention_forward\u001b[0;34m(module, query, key, value, attention_mask, dropout, scaling, is_causal, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m attention_mask\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbool:\n\u001b[1;32m     93\u001b[0m         \u001b[38;5;66;03m# Convert to boolean type, making sdpa to force call FlashAttentionScore to improve performance.\u001b[39;00m\n\u001b[1;32m     94\u001b[0m         attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlogical_not(attention_mask\u001b[38;5;241m.\u001b[39mbool())\u001b[38;5;241m.\u001b[39mto(query\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 96\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msdpa_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# train action head\n",
    "action_head = MLP(LATENT_DIM, ACTION_DIM).to(device)\n",
    "optimizer = torch.optim.AdamW(action_head.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "def report_stats():\n",
    "    sync()\n",
    "    elapsed = time.perf_counter() - t0\n",
    "    sps = i / max(1e-9, elapsed)\n",
    "    bps = n / max(1e-9, elapsed)\n",
    "    eta = (num_batches - i) / max(1e-9, sps)\n",
    "    print(\n",
    "        f\"Epoch {epoch}: step {i}/{len(train_loader)} \"\n",
    "        f\"Loss = {loss.item():.4f} \"\n",
    "        f\"steps / sec = {sps:.1f}, batches / sec = {bps:.1f} \"\n",
    "        f\"ETA = {eta / 60.0:.1f} min\"\n",
    "    )\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    action_head.train()\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    n = 0\n",
    "    num_batches = len(train_loader) # could be outside loop\n",
    "    for i, batch in enumerate(train_loader):\n",
    "\n",
    "        attach_goals(batch, train_goals)\n",
    "        z_pix, z_prp, z_act, z_gpix, z_gprp = encode(batch, dinowm, device, USE_ACTIONS)\n",
    "        z = to_feature(z_pix, z_prp, z_act, z_gpix, z_gprp)\n",
    "        action = batch['action'][:,-1,:2].to(device) # first action from the last (current) step\n",
    "\n",
    "        pred = action_head(z)\n",
    "\n",
    "        loss = F.mse_loss(pred, action)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        n += BATCH_SIZE\n",
    "        if i % 100 == 0:\n",
    "            report_stats()\n",
    "    \n",
    "    # eval\n",
    "    action_head.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            attach_goals(batch, val_goals)\n",
    "            z_pix, z_prp, z_act, z_gpix, z_gprp = encode(batch, dinowm, device, USE_ACTIONS)\n",
    "            z = to_feature(z_pix, z_prp, z_act, z_gpix, z_gprp)\n",
    "            action = batch['action'][:,-1,:2].to(device)\n",
    "\n",
    "            pred = action_head(z)\n",
    "            loss = F.mse_loss(pred, action)\n",
    "            val_loss += loss.item() * BATCH_SIZE\n",
    "\n",
    "    val_rmse = math.sqrt(val_loss / len(val_data))\n",
    "    print(f'epoch {epoch}: RMSE: {val_rmse:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670174e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1ba8e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dd0fdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4a8bea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SWM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
