{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "22d18e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "from datasets import load_from_disk, Dataset\n",
    "\n",
    "import stable_worldmodel as swm\n",
    "from stable_worldmodel.data import StepsDataset, dataset_info\n",
    "from stable_worldmodel.policy import AutoCostModel\n",
    "from stable_worldmodel.wm.dinowm import DINOWM\n",
    "\n",
    "import stable_pretraining as spt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import math, time, contextlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a2f7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "NUM_WORKERS = 6\n",
    "NUM_STEPS = 2 # T\n",
    "BATCH_SIZE = 256 # B\n",
    "FRAMESKIP = 5 # S\n",
    "EPOCHS = 25\n",
    "LEARNING_RATE = 3e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "USE_ACTIONS = True\n",
    "\n",
    "# file paths\n",
    "TRAIN_DIR = \"pusht_expert_dataset_train\"\n",
    "VAL_DIR = \"pusht_expert_dataset_val\"\n",
    "CHECKPOINT_NAME = \"dinowm_pusht_object.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "80b6cff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple MLP\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, out_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ff54e519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform function: normalize + reshape\n",
    "def step_transform(num_steps):\n",
    "    transforms = []\n",
    "    for t in range(num_steps):\n",
    "        key = f\"pixels.{t}\"\n",
    "        transforms.append(\n",
    "            spt.data.transforms.Compose(\n",
    "                spt.data.transforms.ToImage(\n",
    "                    mean=[0.5, 0.5, 0.5],\n",
    "                    std=[0.5, 0.5, 0.5],\n",
    "                    source=key,\n",
    "                    target=key,\n",
    "                ),\n",
    "                spt.data.transforms.Resize(224, source=key, target=key),\n",
    "                spt.data.transforms.CenterCrop(224, source=key, target=key),\n",
    "            )\n",
    "        )\n",
    "    return spt.data.transforms.Compose(*transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac2c2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attach_goal(steps: StepsDataset):\n",
    "    data = steps.dataset\n",
    "    if \"goal\" in data.column_names in data.column_names:\n",
    "        return\n",
    "    data = data.with_format(\"python\")\n",
    "\n",
    "    ep_ids = data[\"episode_idx\"]\n",
    "    pixels = data[\"pixels\"]\n",
    "    proprio = data[\"proprio\"]\n",
    "    \n",
    "    last = {}\n",
    "    for idx, ep_id in enumerate(ep_ids):\n",
    "        last[ep_id] = idx\n",
    "\n",
    "    goal_pixels = [pixels[last[ep_id]] for ep_id in ep_ids]\n",
    "    goal_proprio = [proprio[last[ep_id]] for ep_id in ep_ids]\n",
    "    data = data.add_column(\"goal\", goal_pixels)\n",
    "    data = data.add_column(\"goal_proprio\", goal_proprio)\n",
    "\n",
    "    steps.dataset = data.with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22236f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def load_dataset(dir, num_steps=1, frameskip=5):\n",
    "    transform = step_transform(num_steps)\n",
    "\n",
    "    dataset = StepsDataset(dir, num_steps=num_steps, transform=transform, frameskip=frameskip) # frameskip?\n",
    "    # hacky\n",
    "    dataset.data_dir = dataset.data_dir.parent\n",
    "\n",
    "    # attach goal col\n",
    "    attach_goal(dataset)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "64859118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m22:08:57.705\u001b[0m | \u001b[33m\u001b[1mWARNING\u001b[0m (\u001b[36m24996, stable_pretraining.data.datasets\u001b[0m) | \u001b[33m\u001b[1mYou didn't pass a storage optionwe are adding one to avoid timeout\u001b[0m\n",
      "\u001b[32m22:08:57.706\u001b[0m | \u001b[1mINFO   \u001b[0m (\u001b[36m24996, stable_pretraining.data.datasets\u001b[0m) | \u001b[1mLoading dataset with load_from_disk /Users/ashton/.stable_worldmodel/pusht_expert_dataset_train\u001b[0m\n",
      "\u001b[32m22:09:07.486\u001b[0m | \u001b[33m\u001b[1mWARNING\u001b[0m (\u001b[36m24996, stable_pretraining.data.datasets\u001b[0m) | \u001b[33m\u001b[1mYou didn't pass a storage optionwe are adding one to avoid timeout\u001b[0m\n",
      "\u001b[32m22:09:07.486\u001b[0m | \u001b[1mINFO   \u001b[0m (\u001b[36m24996, stable_pretraining.data.datasets\u001b[0m) | \u001b[1mLoading dataset with load_from_disk /Users/ashton/.stable_worldmodel/pusht_expert_dataset_val\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "train_data, val_data = load_dataset(TRAIN_DIR, num_steps=2), load_dataset(VAL_DIR, num_steps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2b69c074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2168571 2325\n",
      "episode_idx\n",
      "torch.Size([2])\n",
      "torch.Size([1, 10])\n",
      "step_idx\n",
      "torch.Size([2])\n",
      "torch.Size([1, 10])\n",
      "action\n",
      "torch.Size([2, 10])\n",
      "torch.Size([1, 10])\n",
      "state\n",
      "torch.Size([2, 5])\n",
      "torch.Size([1, 10])\n",
      "proprio\n",
      "torch.Size([2, 4])\n",
      "torch.Size([1, 10])\n",
      "pixels\n",
      "torch.Size([2, 3, 224, 224])\n",
      "torch.Size([1, 10])\n",
      "sample_idx\n",
      "torch.Size([2])\n",
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data), len(val_data))\n",
    "for col in train_data.column_names:\n",
    "    print(col)\n",
    "    print(train_data[0][col].shape)\n",
    "    actions = train_data[8]['action'][:-1]\n",
    "    print(actions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dd71de68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(train_data, val_data, batch_size, device, num_workers):\n",
    "    # optionally pin_memory on CUDA, not Mac\n",
    "    train_loader = DataLoader(\n",
    "        train_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        persistent_workers=True,\n",
    "        pin_memory=(device.type=='cuda')\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        persistent_workers=True,\n",
    "        pin_memory=(device.type=='cuda')\n",
    "    )\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cb078b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model checkpoint from cache_dir in inference mode\n",
    "def load_checkpoint(checkpoint_name, device):\n",
    "    cache_dir = swm.data.get_cache_dir()\n",
    "    checkpoint_path = cache_dir / checkpoint_name\n",
    "    model = torch.load(checkpoint_path, map_location=device, weights_only=False) # weights_only false\n",
    "    model = model.to(device).eval()\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad_(False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cfd1e7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad 0s in front of action for action_embedding\n",
    "def pad_actions(actions, frame_skip=5):\n",
    "    if frame_skip == 1:\n",
    "        return actions\n",
    "    B, T, d_a = actions.shape\n",
    "    zeros = torch.zeros((B, T, d_a * (frame_skip - 1)))\n",
    "    padded = torch.cat((zeros, actions), dim=2)\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5899d546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_actions(actions: torch.Tensor, frame_skip=5):\n",
    "    repeated = actions.repeat(1,1,frame_skip)\n",
    "    print(actions.shape)\n",
    "    print(repeated.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cd71f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder func: call dinowm encoders\n",
    "@torch.inference_mode()\n",
    "def encode(batch, dinowm, device, use_actions=False):\n",
    "    if device.type in (\"cuda\", \"mps\"):\n",
    "        context = torch.autocast(device_type=device.type, dtype=torch.float16) # float32?\n",
    "    else:\n",
    "        context = contextlib.nullcontext()\n",
    "    \n",
    "    # no pad, we use frameskip=5\n",
    "    # actions = batch[\"action\"][::-1]\n",
    "    # print(actions.shape)\n",
    "    # padded_actions = pad_actions(actions)\n",
    "    # repeated_actions = repeat_actions(actions)\n",
    "    # print(repeated_actions.shape)\n",
    "\n",
    "    data = {\n",
    "        \"pixels\":  batch[\"pixels\"].to(device, non_blocking=True),\n",
    "        \"proprio\": batch[\"proprio\"].to(device, non_blocking=True),\n",
    "    }\n",
    "    if use_actions:\n",
    "        # drop last block of frame_skip actions\n",
    "        actions = batch['action'][:,:-1] # B x (T - 1) * d_actions_effective := (d_actions * frame_skip)\n",
    "        data[\"action\"] = actions.to(device, non_blocking=True)\n",
    "\n",
    "    with context:\n",
    "        out = dinowm.encode(\n",
    "            data,\n",
    "            target=\"embed\",\n",
    "            pixels_key=\"pixels\",\n",
    "            proprio_key=\"proprio\",\n",
    "            action_key=(\"action\" if use_actions else None),\n",
    "        )\n",
    "\n",
    "    z_pixels = out[\"pixels_embed\"].mean(dim=2).float() # B x T x d_pixels (pooled by patch)\n",
    "    z_proprio = out[\"proprio_embed\"].float() # B x T x d_proprio\n",
    "    z_actions = (out[\"action_embed\"].float() if use_actions else None) # B x (T-1) x d_actions_effective\n",
    "    return z_pixels, z_proprio, z_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46e05b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_feature(z_pixels, z_proprio, z_actions):\n",
    "    parts = [z_pixels[:,:-1], z_proprio[:,:-1]]\n",
    "    if z_actions is not None:\n",
    "        parts.append(z_actions)\n",
    "\n",
    "    # history latents (block of S actions + state := (pixel + proprio embeddings))\n",
    "    z_hist = torch.cat(parts, dim=2) # B x (T-1) x d_embed := [d_pixels + d_proprio (+ d_actions_effective)]\n",
    "    z_hist = torch.flatten(z_hist, start_dim=1, end_dim=2) # B x ((T-1) * d_embed)\n",
    "    \n",
    "    # current latents (just pixel + proprio embeedings)\n",
    "    z_cur = torch.cat((z_pixels[:,-1], z_proprio[:,-1]), dim=2) # B x 1 x (d_pixels + d_proprio)\n",
    "    z_cur = torch.flatten(z_cur, start_dim=1, end_dim=2) # B x ((T - 1) * d_embed)\n",
    "\n",
    "    # concat\n",
    "    z = torch.cat((z_hist, z_cur), dim=1)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2e628f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: mps\n",
      "\u001b[32m01:01:09.410\u001b[0m | \u001b[33m\u001b[1mWARNING\u001b[0m (\u001b[36m24996, stable_pretraining.data.datasets\u001b[0m) | \u001b[33m\u001b[1mYou didn't pass a storage optionwe are adding one to avoid timeout\u001b[0m\n",
      "\u001b[32m01:01:09.410\u001b[0m | \u001b[1mINFO   \u001b[0m (\u001b[36m24996, stable_pretraining.data.datasets\u001b[0m) | \u001b[1mLoading dataset with load_from_disk /Users/ashton/.stable_worldmodel/pusht_expert_dataset_train\u001b[0m\n",
      "\u001b[32m01:01:19.693\u001b[0m | \u001b[33m\u001b[1mWARNING\u001b[0m (\u001b[36m24996, stable_pretraining.data.datasets\u001b[0m) | \u001b[33m\u001b[1mYou didn't pass a storage optionwe are adding one to avoid timeout\u001b[0m\n",
      "\u001b[32m01:01:19.694\u001b[0m | \u001b[1mINFO   \u001b[0m (\u001b[36m24996, stable_pretraining.data.datasets\u001b[0m) | \u001b[1mLoading dataset with load_from_disk /Users/ashton/.stable_worldmodel/pusht_expert_dataset_val\u001b[0m\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'USE_ACTIONS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 92\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: RMSE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_rmse\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 92\u001b[0m     \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[54], line 32\u001b[0m, in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m d_proprio \u001b[38;5;241m=\u001b[39m dinowm\u001b[38;5;241m.\u001b[39mproprio_encoder\u001b[38;5;241m.\u001b[39memb_dim\n\u001b[1;32m     31\u001b[0m d_action \u001b[38;5;241m=\u001b[39m dinowm\u001b[38;5;241m.\u001b[39maction_encoder\u001b[38;5;241m.\u001b[39memb_dim\n\u001b[0;32m---> 32\u001b[0m LATENT_DIM \u001b[38;5;241m=\u001b[39m d_pixel \u001b[38;5;241m+\u001b[39m d_proprio \u001b[38;5;241m+\u001b[39m (d_action \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mUSE_ACTIONS\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     33\u001b[0m ACTION_DIM \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;66;03m# predict actual action, not latent\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatent_dim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mLATENT_DIM\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, action_dim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACTION_DIM\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'USE_ACTIONS' is not defined"
     ]
    }
   ],
   "source": [
    "def run():\n",
    "    # find device\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    def sync():\n",
    "        if device.type=='cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        if device.type=='mps':\n",
    "            torch.mps.synchronize()\n",
    "\n",
    "    print(\"device:\", device)\n",
    "\n",
    "    # load datasets\n",
    "    train_data, val_data = load_dataset(TRAIN_DIR, NUM_STEPS), load_dataset(VAL_DIR, NUM_STEPS)\n",
    "\n",
    "    # build loaders\n",
    "    train_loader, val_loader = get_loaders(train_data, val_data, BATCH_SIZE, device, NUM_WORKERS)\n",
    "\n",
    "    # load DINO-WM\n",
    "    dinowm = load_checkpoint(CHECKPOINT_NAME, device)\n",
    "    assert isinstance(dinowm, DINOWM)\n",
    "        \n",
    "    # calculate dims\n",
    "    d_pixel = dinowm.backbone.config.hidden_size\n",
    "    d_proprio = dinowm.proprio_encoder.emb_dim\n",
    "    d_action = dinowm.action_encoder.emb_dim\n",
    "    LATENT_DIM = d_pixel + d_proprio + (d_action if USE_ACTIONS else 0)\n",
    "    ACTION_DIM = 2 # predict actual action, not latent\n",
    "    print(f'latent_dim={LATENT_DIM}, action_dim={ACTION_DIM}')\n",
    "\n",
    "    # train action head\n",
    "    action_head = MLP(LATENT_DIM, ACTION_DIM).to(device)\n",
    "    optimizer = torch.optim.AdamW(action_head.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    def report_stats():\n",
    "        sync()\n",
    "        elapsed = time.perf_counter() - t0\n",
    "        sps = i / max(1e-9, elapsed)\n",
    "        bps = n / max(1e-9, elapsed)\n",
    "        eta = (num_batches - i) / max(1e-9, sps)\n",
    "        print(\n",
    "            f\"Epoch: {epoch} Step: {i}/{len(train_loader)} \"\n",
    "            f\"Loss = {loss.item():.4f} \"\n",
    "            f\"steps / sec = {sps:.1f}, samples / sec = {bps:.1f} \"\n",
    "            f\"ETA = {eta/60:.1f} min\"\n",
    "        )\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        action_head.train()\n",
    "\n",
    "        t0 = time.perf_counter()\n",
    "        n = 0\n",
    "        num_batches = len(train_loader) # could be outside loop\n",
    "\n",
    "        for i, batch in enumerate(train_loader):\n",
    "\n",
    "            z_pix, z_prp, z_act = encode(batch, dinowm, device, USE_ACTIONS)\n",
    "            z = to_feature(z_pix, z_prp, z_act)\n",
    "            action = batch['action'][:,-1,:2].to(device) # first action from the last (current) step\n",
    "\n",
    "            pred = action_head(z)\n",
    "\n",
    "            loss = F.mse_loss(pred, action)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if i % 50 == 0:\n",
    "                report_stats()\n",
    "            n += BATCH_SIZE\n",
    "        \n",
    "        # eval\n",
    "        action_head.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                z_pix, z_prp, z_act = encode(batch, USE_ACTIONS)\n",
    "                z = to_feature(z_pix, z_prp, z_act)\n",
    "                action = batch['action'][:,-1,:2].to(device)\n",
    "\n",
    "                z = action_head(z)\n",
    "                val_loss += F.mse_loss(pred, action)\n",
    "        val_rmse = math.sqrt(val_loss / len(val_data))\n",
    "        print(f'epoch {epoch}: RMSE: {val_rmse:.6f}')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb718b7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SWM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
