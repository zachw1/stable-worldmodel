{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca335d1a",
   "metadata": {},
   "source": [
    "TODO:\n",
    "\n",
    "This is for prototyping\n",
    "\n",
    "I will turn this into a example_action_head.py which loads the StepsDataset then trains the MLP and saves the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9adca16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, Dataset\n",
    "import stable_worldmodel as swm\n",
    "from stable_worldmodel.data import StepsDataset\n",
    "from stable_worldmodel.policy import AutoCostModel\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import stable_pretraining as spt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "083de23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = \"~/.stable_worldmodel\"  # should be default\n",
    "\n",
    "train_dir = \"pusht_expert_dataset_train\"\n",
    "val_dir = \"pusht_expert_dataset_val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad50c95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = f'{cache_dir}/{train_dir}/'\n",
    "val_path   = f'{cache_dir}/{val_dir}/'\n",
    "\n",
    "ds_train = load_from_disk(train_path)\n",
    "ds_val   = load_from_disk(val_path)\n",
    "# ds_train = Dataset.from_file(f\"{train_dir}/data-00000-of-00001.arrow\")\n",
    "# ds_val   = Dataset.from_file(f\"{val_dir}/data-00000-of-00001.arrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa4a8326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'episode_idx': Value('int64'), 'step_idx': Value('int64'), 'action': List(Value('float64')), 'state': List(Value('float64')), 'proprio': List(Value('float64')), 'pixels': Value('string')}\n",
      "{'episode_idx': 0, 'step_idx': 0, 'action': [0.14000000059604645, -0.25], 'state': [423.0, 182.0, 192.6706085205078, 335.1567687988281, 2.947699785232544], 'proprio': [423.0, 182.0, 0.0, 0.0], 'pixels': 'pusht_expert_dataset_train/img/0/0_pixels.jpeg'}\n"
     ]
    }
   ],
   "source": [
    "print(ds_train.features)\n",
    "print(ds_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ab9402d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m01:33:35.063\u001b[0m | \u001b[33m\u001b[1mWARNING\u001b[0m (\u001b[36m10074, stable_pretraining.data.datasets\u001b[0m) | \u001b[33m\u001b[1mYou didn't pass a storage optionwe are adding one to avoid timeout\u001b[0m\n",
      "\u001b[32m01:33:35.064\u001b[0m | \u001b[1mINFO   \u001b[0m (\u001b[36m10074, stable_pretraining.data.datasets\u001b[0m) | \u001b[1mLoading dataset with load_from_disk /Users/ashton/.stable_worldmodel/pusht_expert_dataset_train\u001b[0m\n",
      "\u001b[32m01:33:44.416\u001b[0m | \u001b[33m\u001b[1mWARNING\u001b[0m (\u001b[36m10074, stable_pretraining.data.datasets\u001b[0m) | \u001b[33m\u001b[1mYou didn't pass a storage optionwe are adding one to avoid timeout\u001b[0m\n",
      "\u001b[32m01:33:44.417\u001b[0m | \u001b[1mINFO   \u001b[0m (\u001b[36m10074, stable_pretraining.data.datasets\u001b[0m) | \u001b[1mLoading dataset with load_from_disk /Users/ashton/.stable_worldmodel/pusht_expert_dataset_val\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "NUM_STEPS = 1\n",
    "train_data = StepsDataset(train_dir, num_steps = NUM_STEPS)\n",
    "val_data = StepsDataset(val_dir, num_steps = NUM_STEPS)\n",
    "\n",
    "# BUG FIX -> arrow shard pointed paths wrong\n",
    "for df in (train_data, val_data):\n",
    "    df.data_dir = df.data_dir.parent\n",
    "\n",
    "\n",
    "# print(data_train.infer_img_path_columns())\n",
    "\n",
    "# ERROR -> stable-pretraining uses load_dataset, not load_from_disk\n",
    "# FIX -> update stable-pretraining and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "59ce4149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m01:21:19.957\u001b[0m | \u001b[33m\u001b[1mWARNING\u001b[0m (\u001b[36m10074, stable_pretraining.data.datasets\u001b[0m) | \u001b[33m\u001b[1mYou didn't pass a storage optionwe are adding one to avoid timeout\u001b[0m\n",
      "\u001b[32m01:21:19.958\u001b[0m | \u001b[1mINFO   \u001b[0m (\u001b[36m10074, stable_pretraining.data.datasets\u001b[0m) | \u001b[1mLoading dataset with load_from_disk /Users/ashton/.stable_worldmodel/pusht_expert_dataset_train\u001b[0m\n",
      "\u001b[32m01:21:29.330\u001b[0m | \u001b[33m\u001b[1mWARNING\u001b[0m (\u001b[36m10074, stable_pretraining.data.datasets\u001b[0m) | \u001b[33m\u001b[1mYou didn't pass a storage optionwe are adding one to avoid timeout\u001b[0m\n",
      "\u001b[32m01:21:29.331\u001b[0m | \u001b[1mINFO   \u001b[0m (\u001b[36m10074, stable_pretraining.data.datasets\u001b[0m) | \u001b[1mLoading dataset with load_from_disk /Users/ashton/.stable_worldmodel/pusht_expert_dataset_val\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def step_transform():\n",
    "    transforms = []\n",
    "    for t in range(NUM_STEPS):\n",
    "        key = f\"pixels.{t}\"\n",
    "        transforms.append(\n",
    "            spt.data.transforms.Compose(\n",
    "                spt.data.transforms.ToImage(\n",
    "                    mean=[0.5, 0.5, 0.5],\n",
    "                    std=[0.5, 0.5, 0.5],\n",
    "                    source=key,\n",
    "                    target=key,\n",
    "                ),\n",
    "                spt.data.transforms.Resize(224, source=key, target=key),\n",
    "                spt.data.transforms.CenterCrop(224, source=key, target=key),\n",
    "            )\n",
    "        )\n",
    "    return spt.data.transforms.Compose(*transforms)\n",
    "\n",
    "transform = step_transform()\n",
    "train_data = StepsDataset(\"pusht_expert_dataset_train\", num_steps=1, transform=transform)\n",
    "val_data   = StepsDataset(\"pusht_expert_dataset_val\",   num_steps=1, transform=transform)\n",
    "\n",
    "for df in (train_data, val_data):\n",
    "    df.data_dir = df.data_dir.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "39ff47a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# print(train_data[0])\n",
    "print(train_data[0]['pixels'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c3a178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ashton/.stable_worldmodel/dinowm_object.ckpt\n"
     ]
    }
   ],
   "source": [
    "dinowm = AutoCostModel(model_name=\"dinowm\") # no _object\n",
    "# this is just cost_head?\n",
    "dinowm.eval().requires_grad_(False)\n",
    "\n",
    "# print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "446d1622",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "cache_dir = swm.data.get_cache_dir()  \n",
    "\n",
    "# torch load better?\n",
    "checkpoint_name = \"dinowm_object.ckpt\"\n",
    "\n",
    "checkpoint = cache_dir / checkpoint_name\n",
    "dinowm = torch.load(checkpoint, map_location=device, weights_only=False)\n",
    "dinowm = dinowm.to(device).eval()\n",
    "\n",
    "for p in dinowm.parameters():\n",
    "    p.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8afa833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384 2\n"
     ]
    }
   ],
   "source": [
    "LATENT_DIM = dinowm.backbone.config.hidden_size\n",
    "ACTION_DIM = len(train_data.dataset['action'][0])\n",
    "\n",
    "# action = (x, y)\n",
    "# print(LATENT_DIM, ACTION_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a81c44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms\n",
    "\n",
    "# from train/dinowm.py -> use 'pixels.0' for step size 1\n",
    "# all the expert data should all be 224x224 already\n",
    "transform = spt.data.transforms.Compose(\n",
    "    spt.data.transforms.ToImage(mean=[0.5, 0.5, 0.5],\n",
    "                                std=[0.5, 0.5, 0.5],\n",
    "                                source=\"pixels.0\",\n",
    "                                target=\"pixels.0\"),\n",
    "    spt.data.transforms.Resize(224, source=\"pixels.0\", target=\"pixels.0\"),\n",
    "    spt.data.transforms.CenterCrop(224, source=\"pixels.0\", target=\"pixels.0\"),\n",
    ")\n",
    "\n",
    "def batch_transform(batch):\n",
    "    batch[\"pixels\"] = torch.stack([transform({\"pixels.0\": img})[\"pixels.0\"] for img in batch[\"pixels\"]])\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bf1927",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_dict = {'pixels': batch_pixels, }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc9d65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 64\n",
    "ACTION_DIM = 32\n",
    "EXPANSION_FACTOR = 2\n",
    "\n",
    "\n",
    "# simple action head, MLP\n",
    "# upproject one time\n",
    "# can make take cfgs later\n",
    "\n",
    "class ActionHead(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim):\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "# From DINO-WM implementation:\n",
    "\n",
    "# class FeedForward(nn.Module):\n",
    "#     def __init__(self, dim, hidden_dim, dropout=0.0):\n",
    "#         super().__init__()\n",
    "#         self.net = nn.Sequential(\n",
    "#             nn.LayerNorm(dim),\n",
    "#             nn.Linear(dim, hidden_dim),\n",
    "#             nn.GELU(),\n",
    "#             nn.Dropout(dropout),\n",
    "#             nn.Linear(hidden_dim, dim),\n",
    "#             nn.Dropout(dropout),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.net(x)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SWM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
