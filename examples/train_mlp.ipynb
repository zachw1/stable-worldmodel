{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca335d1a",
   "metadata": {},
   "source": [
    "TODO:\n",
    "\n",
    "This is for prototyping\n",
    "\n",
    "I will turn this into a example_action_head.py which loads the StepsDataset then trains the MLP and saves the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adca16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, Dataset\n",
    "\n",
    "import stable_worldmodel as swm\n",
    "from stable_worldmodel.data import StepsDataset\n",
    "from stable_worldmodel.policy import AutoCostModel\n",
    "from stable_worldmodel.wm.dinowm import DINOWM\n",
    "\n",
    "import stable_pretraining as spt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083de23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache_dir = \"~/.stable_worldmodel\"\n",
    "cache_dir = swm.data.get_cache_dir()  \n",
    "\n",
    "train_dir = \"pusht_expert_dataset_train\"\n",
    "val_dir = \"pusht_expert_dataset_val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad50c95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = cache_dir / train_dir\n",
    "val_path   = cache_dir / val_dir\n",
    "\n",
    "ds_train = load_from_disk(train_path)\n",
    "ds_val   = load_from_disk(val_path)\n",
    "# ds_train = Dataset.from_file(f\"{train_dir}/data-00000-of-00001.arrow\")\n",
    "# ds_val   = Dataset.from_file(f\"{val_dir}/data-00000-of-00001.arrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4a8326",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds_train.features)\n",
    "print(ds_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ce4149",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_STEPS = 1\n",
    "\n",
    "def step_transform():\n",
    "    transforms = []\n",
    "    for t in range(NUM_STEPS):\n",
    "        key = f\"pixels.{t}\"\n",
    "        transforms.append(\n",
    "            spt.data.transforms.Compose(\n",
    "                spt.data.transforms.ToImage(\n",
    "                    mean=[0.5, 0.5, 0.5],\n",
    "                    std=[0.5, 0.5, 0.5],\n",
    "                    source=key,\n",
    "                    target=key,\n",
    "                ),\n",
    "                spt.data.transforms.Resize(224, source=key, target=key),\n",
    "                spt.data.transforms.CenterCrop(224, source=key, target=key),\n",
    "            )\n",
    "        )\n",
    "    return spt.data.transforms.Compose(*transforms)\n",
    "\n",
    "transform = step_transform()\n",
    "train_data = StepsDataset(\"pusht_expert_dataset_train\", num_steps=NUM_STEPS, transform=transform)\n",
    "val_data   = StepsDataset(\"pusht_expert_dataset_val\",   num_steps=NUM_STEPS, transform=transform)\n",
    "\n",
    "for df in (train_data, val_data):\n",
    "    df.data_dir = df.data_dir.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ff47a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_data[0])\n",
    "print(train_data[0]['pixels'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935d0c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "# optionally pin_memory on CUDA, not Mac\n",
    "train_loader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    persistent_workers=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c3a178",
   "metadata": {},
   "outputs": [],
   "source": [
    "dinowm = AutoCostModel(model_name=\"dinowm\") # no _object\n",
    "# this is just cost_head?\n",
    "dinowm.eval().requires_grad_(False)\n",
    "\n",
    "# print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446d1622",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# torch load better?\n",
    "checkpoint_name = \"dinowm_pusht_object.ckpt\"\n",
    "\n",
    "checkpoint = cache_dir / checkpoint_name\n",
    "dinowm = torch.load(checkpoint, map_location=device, weights_only=False)\n",
    "dinowm = dinowm.to(device).eval()\n",
    "\n",
    "for p in dinowm.parameters():\n",
    "    p.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c9a31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no proprio encoder for now?\n",
    "def encode(batch):\n",
    "    info_d = {\"pixels\": batch[\"pixels\"].to(device), \"proprio\": batch[\"proprio\"].to(device)}\n",
    "    with torch.no_grad():\n",
    "        info_d = dinowm.encode(\n",
    "            info_d,\n",
    "            target=\"embed\",\n",
    "            pixels_key= \"pixels\",\n",
    "            proprio_key=\"proprio\")\n",
    "    # Bx(d_pixels + d_proprio)?\n",
    "    return info_d[\"embed\"][:,-1].mean(dim=1) # last step, mean across patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d544350d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in enumerate(train_loader):\n",
    "    if i ==1:\n",
    "        break\n",
    "    print(batch[\"pixels\"].shape)\n",
    "    latent = encode(batch)\n",
    "    print(batch[\"proprio\"].shape)\n",
    "    print(latent.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc9d65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, out_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8afa833",
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIM = dinowm.backbone.config.hidden_size + dinowm.proprio_encoder.emb_dim\n",
    "ACTION_DIM = len(train_data.dataset['action'][0])\n",
    "\n",
    "# action = (x, y)\n",
    "print(LATENT_DIM, ACTION_DIM)\n",
    "action_head = MLP(LATENT_DIM, ACTION_DIM).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65364e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(action_head.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "\n",
    "EPOCHS = 25\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    # train\n",
    "    action_head.train()\n",
    "    for batch in train_loader:\n",
    "        latent = encode(batch)\n",
    "        action = batch['action'][:,0].to(device)\n",
    "        pred = action_head(latent)\n",
    "        loss = F.mse_loss(pred, action)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    \n",
    "    # eval\n",
    "    action_head.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            latent = encode(batch)\n",
    "            action = batch['action'][:,0].to(device)\n",
    "            pred = action_head(latent)\n",
    "            val_loss += F.mse_loss(pred, action)\n",
    "    val_rmse = math.sqrt(val_loss / len(val_data))\n",
    "    print(f'epoch {epoch}: RMSE: {val_rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a81c44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms\n",
    "\n",
    "# from train/dinowm.py -> use 'pixels.0' for step size 1\n",
    "# all the expert data should all be 224x224 already\n",
    "transform = spt.data.transforms.Compose(\n",
    "    spt.data.transforms.ToImage(mean=[0.5, 0.5, 0.5],\n",
    "                                std=[0.5, 0.5, 0.5],\n",
    "                                source=\"pixels.0\",\n",
    "                                target=\"pixels.0\"),\n",
    "    spt.data.transforms.Resize(224, source=\"pixels.0\", target=\"pixels.0\"),\n",
    "    spt.data.transforms.CenterCrop(224, source=\"pixels.0\", target=\"pixels.0\"),\n",
    ")\n",
    "\n",
    "def batch_transform(batch):\n",
    "    batch[\"pixels\"] = torch.stack([transform({\"pixels.0\": img})[\"pixels.0\"] for img in batch[\"pixels\"]])\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999b3627",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SWM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
